# Copyright The OpenTelemetry Authors
# SPDX-License-Identifier: Apache-2.0

receivers:
  otlp:
    protocols:
      grpc:
        endpoint: ${env:OTEL_COLLECTOR_HOST}:${env:OTEL_COLLECTOR_PORT_GRPC}
      http:
        endpoint: ${env:OTEL_COLLECTOR_HOST}:${env:OTEL_COLLECTOR_PORT_HTTP}
        cors:
          allowed_origins:
            - "http://*"
            - "https://*"
  httpcheck/frontend-proxy:
    targets:
      - endpoint: http://${env:FRONTEND_PROXY_ADDR}
  nginx:
    endpoint: http://${env:IMAGE_PROVIDER_HOST}:${env:IMAGE_PROVIDER_PORT}/status
    collection_interval: 10s

  docker_stats:
    # does not work on RedHat with podman (gives 'context canceled' and parse errors)
    endpoint: "unix:///var/run/docker.sock"
    #    #endpoint: "tcp://host.docker.internal:2375"
    collection_interval: 10s # (default = 10s)
    initial_delay: 20s # (default = 1s)
    timeout: 5s # (default = 5s)
    metrics:
      #	container.cpu.limit: # could not be enabled (?), gives config error on startup
      #           enabled: true
      container.cpu.utilization:
        enabled: true
      container.cpu.logical.count:
        enabled: true
      container.uptime:
        enabled: true
      container.restarts:
        enabled: true
    # https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/44511
    api_version: "1.44"
  postgresql:
    endpoint: ${POSTGRES_HOST}:${POSTGRES_PORT}
    username: root
    password: ${POSTGRES_PASSWORD}
    metrics:
      postgresql.blks_hit:
        enabled: true
      postgresql.blks_read:
        enabled: true
      postgresql.tup_fetched:
        enabled: true
      postgresql.tup_returned:
        enabled: true
      postgresql.tup_inserted:
        enabled: true
      postgresql.tup_updated:
        enabled: true
      postgresql.tup_deleted:
        enabled: true
      postgresql.deadlocks:
        enabled: true
    tls:
      insecure: true
  redis:
    endpoint: "valkey-cart:6379"
    username: "valkey"
    collection_interval: 10s
  # Host metrics
  hostmetrics:
    root_path: /hostfs
    scrapers:
      cpu:
        metrics:
          system.cpu.utilization:
            enabled: true
          system.cpu.logical.count:
            enabled: true
      disk:
      load:
      filesystem:
        exclude_mount_points:
          mount_points:
            - /dev/*
            - /proc/*
            - /sys/*
            - /run/k3s/containerd/*
            - /var/lib/docker/*
            - /var/lib/kubelet/*
            - /snap/*
          match_type: regexp
        exclude_fs_types:
          fs_types:
            - autofs
            - binfmt_misc
            - bpf
            - cgroup2
            - configfs
            - debugfs
            - devpts
            - devtmpfs
            - fusectl
            - hugetlbfs
            - iso9660
            - mqueue
            - nsfs
            - overlay
            - proc
            - procfs
            - pstore
            - rpc_pipefs
            - securityfs
            - selinuxfs
            - squashfs
            - sysfs
            - tracefs
          match_type: strict
      memory:
        metrics:
          system.memory.utilization:
            enabled: true
          system.memory.limit:
            enabled: true
      network:
      paging:
        metrics:
          system.paging.usage:
            enabled: true

      processes:
      process:
        mute_process_exe_error: true
        mute_process_io_error: true
        mute_process_user_error: true
      system:
        metrics:
          system.uptime:
            enabled: true
exporters:
  debug:
  otlp:
    endpoint: "jaeger:4317"
    tls:
      insecure: true
    sending_queue:
      batch:
  otlphttp/prometheus:
    endpoint: "http://prometheus:9090/api/v1/otlp"
    tls:
      insecure: true
    sending_queue:
      batch:
  opensearch:
    logs_index: otel-logs
    logs_index_time_format: "yyyy-MM-dd"
    http:
      endpoint: "http://opensearch:9200"
      tls:
        insecure: true
    sending_queue:
      # Explicitly set due to bug: https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/45016
      num_consumers: 10
      queue_size: 1000
      batch:
  # otlp/qryn:
  #   endpoint: "otel-collector-qryn:4317"
  #   tls:
  #     insecure: true
  otlphttp/qryn:
    endpoint: "http://gigapipe:3100"
    tls:
      insecure: true    
  otlp/tempo:
    endpoint: tempo:4317
    tls:
      insecure: true    
  prometheusremotewrite/qryn:
    endpoint: "http://gigapipe:3100/api/v1/prom/remote/write"
    tls:
      insecure: true
    resource_to_telemetry_conversion:
      enabled: true # Convert resource attributes to metric labels    
processors:
  memory_limiter:
    check_interval: 5s
    limit_percentage: 80
    spike_limit_percentage: 25
  resourcedetection:
    detectors: [env, docker, system]    
  transform:
    error_mode: ignore
    trace_statements:
      - context: span
        statements:
          # could be removed when https://github.com/vercel/next.js/pull/64852 is fixed upstream
          - replace_pattern(name, "\\?.*", "")
          - replace_match(name, "GET /api/products/*", "GET /api/products/{productId}")

  transform/loglevel:
      error_mode: ignore
      log_statements:
        - context: log
          statements:
          - set(log.attributes["sy_loglevel"], "unknown")
          - set(log.attributes["sy_loglevel"], "fatal") where log.severity_number <= SEVERITY_NUMBER_FATAL4 and log.severity_number > 0
          - set(log.attributes["sy_loglevel"], "error") where log.severity_number <= SEVERITY_NUMBER_ERROR4 and log.severity_number > 0
          - set(log.attributes["sy_loglevel"], "warn") where log.severity_number <= SEVERITY_NUMBER_WARN4   and log.severity_number > 0
          - set(log.attributes["sy_loglevel"], "info") where log.severity_number <= SEVERITY_NUMBER_INFO4   and log.severity_number > 0
          - set(log.attributes["sy_loglevel"], "debug") where log.severity_number <= SEVERITY_NUMBER_DEBUG4 and log.severity_number > 0
          - set(log.attributes["sy_loglevel"], "trace") where log.severity_number <= SEVERITY_NUMBER_TRACE4 and log.severity_number > 0
          # for Loki: set level explicitly
          - set(log.attributes["level"], log.attributes["sy_loglevel"])

connectors:
  spanmetrics:

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [resourcedetection, memory_limiter, transform]
      exporters: [otlp, otlphttp/qryn, otlp/tempo, spanmetrics, opensearch, debug]
      # exporting traces to OpenSearch does not work reliably with OpenSearch exporter
      # get errors like "can't merge a non object mapping [attributes.upstream_cluster] with an object mapping"
      # or "object mapping for [attributes.user_agent] tried to parse field [user_agent] as object, but found a concrete value"
    metrics:
      receivers: [docker_stats, httpcheck/frontend-proxy, hostmetrics, nginx, otlp, redis, spanmetrics]
      processors: [resourcedetection, memory_limiter, batch]
      exporters: [otlphttp/prometheus, prometheusremotewrite/qryn, debug]
    logs:
      receivers: [otlp]
      processors: [resourcedetection, transform/loglevel,memory_limiter, batch]
      exporters: [opensearch, otlphttp/qryn, debug]
  telemetry:
    metrics:
      level: detailed
      readers:
        - periodic:
            interval: 10000
            timeout: 5000
            exporter:
              otlp:
                protocol: http/protobuf
                endpoint: http://${env:OTEL_COLLECTOR_HOST}:${env:OTEL_COLLECTOR_PORT_HTTP}
